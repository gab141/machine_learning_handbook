\chapter{User Matrix in Recommendation Systems}\label{ch:user-matrix}

\index{Matrix Factorisation|(}

Matrix Factorization algorithms are generally classified as a collaborative filtering algorithms that are generally used in User Recommendation Systems. This type of algorithm generally works by performing a decomposition of the interaction matrix between the user and the item into the product of two lower dimensionality rectangular matrices \citep{Koren2006},

Modern day consumers are spoilt for choice as content providers offer a massive selection of products and options which can cater to a wide array of interests and tastes. Matching the appropriate product to the right customer is essential to improve client satisfaction, for that reason recommender systems have dramatically increased in popularity as these systems analyse patterns and previous choices of users to determine areas of interest which allows the content providers to provide a personalised experience. Many of the large internet-based companies such as Youtube, Amazon and Ebay are using these systems to add another dimension to the experience the user is receiving on these types of websites.
\section{Matrix Factorisation}
\index{Ensemble methods!bagging|(}
In a simple form matrix factorization classifies both items and users by vectors of factors will be extracted from user item rating patterns. High correlation between items and user factors will lead to a more accurate and specific recommendation. These techniques have become more common in recent years as they improve the overall user experience with a website. This type of system requires different types of input data, generally this data is split into two dimensions, one of these dimensions represents Users and the other represents items of interest. The ideal data for items of interest involves in depth feedback and ratings from users to provide a more ideal recommendation as it directly reflects the opinion of the user using a system. One of the main strong points of using a matrix in this case is that it allows the incorporation of additional data to be combined within this matrix to provide greater correlation. 
\begin{equation}\label{eq:baggingvote}
    f(x) = argmax \sum_{b=1}^{B} f_b(y | x)
\end{equation}

\begin{equation}\label{eq:baggingmean}
    f(x) = \frac{1}{B} \sum_{b=1}^{B} f_b(x)
\end{equation}

\begin{figure}
  \includegraphics{graphics/user_matrix/usermatrix.JPG}
  \caption{
    Dataset Bagging using 3 base learners. Each bag is a subset of the training set. Instances are selected randomly and with replacement. Each base learner is fitted with the respective bag. Predictions are then made by each base learner and the final output $\hat{y}$ is the maximum vote or an averaged value of all predictions. 
  }
  \label{fig:baggingexample}
\end{figure}

\textit{Bagging} can also be applied to the feature set where subsets of the features is selected at random and used to fit the base learners. This technique is referred to as \textit{Feature Bagging}\index{Ensemble methods!bagging!feature bagging}. One popular model that utilizes \textit{Feature Bagging} is \textit{Random Forests}\index{Ensemble methods!bagging!random forests} introduced by \citet{ho1995random}. It uses multiple \textit{Decision Trees}\index{Ensemble methods!bagging!decision trees} which are trained using subsets of the total feature set selected at random. It was shown that when applying this method with multiple \textit{Decision Trees}, the model was able to generalize more when classifying handwritten digits. An extension to \textit{Random Forests} was later developed which uses both \textit{Bagging} and \textit{Feature Bagging} to further control variance \citep{breiman2001random}.

\index{Ensemble methods!bagging|)}

\section{Boosting}
\index{Ensemble methods!boosting|(}
This technique is a variation on \textit{Bagging} which aims to improve the learner's predictive performance by correcting the errors made by the previous learners in the ensemble, hence the name \textit{Boosting}. The idea of having multiple \textit{weak learners}\index{Ensemble methods!boosting!weak learners} that adapt by correcting errors was introduced by \citet{freund1997decision} and this technique is called \textit{Adaptive Boosting}\index{Ensemble methods!boosting!adaptive boosting} or \textit{AdaBoost}, which is the most common \textit{Boosting} method. It was shown that this technique can reduce both bias and variance \citep{breiman1996bias}. \textit{AdaBoost} does not work well when having insufficient data or very weak hypotheses \citep{freund1999short}. It also suffers when having a large number of outliers in the dataset and can significantly reduce the predictive performance \citep{dietterich1998experimental}. Variants of \textit{AdaBoost} were developed to combat large amount of outliers in the dataset such as \textit{'Gentle AdaBoost'}\index{Ensemble methods!boosting!gentle adaboost} which gives less importance to outliers \citep{friedman2000additive} and \textit{'BrownBoost'}\index{Ensemble methods!boosting!brown boost} which uses a combination of Brownain motion, \textit{Boosting} and repeated games to handle such cases \citep{freund2001adaptive}.  

\textit{Boosting} works by taking a subset of the training set to train the first learner. Once the learner is trained, it is validated using the whole training set. When validation is done, each instance in the original dataset is given a weight based on the error made by this learner. A new learner is now trained on a new subset, but this time instances with higher weights are more likely to be selected. Again, this learner is validated using the whole training set but the weights for each instance is now recalibrated using a combined prediction of the two learners as shown in Figure~\ref{fig:boostingexample}. This process goes on for the number of learners specified to be used in the ensemble and the idea behind it is that each learner corrects the previous learner's errors.  Once this procedure is finished, a prediction is made on the weighted mean of each learner if it is a regression problem as shown in Equation~\ref{eq:boostinggmean}. If it is a classification problem, it takes a weighted vote to come up with the predicted classification  as shown in Equation~\ref{eq:boostingvote}. 

\begin{figure}
  \includegraphics{graphics/ensemble_methods/boosting_example.JPG}
  \caption{
     Boosting using 2 base learners. Initially weights are set to 1, so all instances have the same probability to be selected in the subset. The first learner is trained using the first subset and then validated using the whole training set. Weights are recalibrated depending on the errors of the predictions. On the second run, instances which had significant errors are more likely to be selected. The second learner is now trained and tested using the whole training set. This time weight are adjusted using the combined predictions of the two learners.  
  }
  \label{fig:boostingexample}
\end{figure}

\begin{equation}\label{eq:boostinggmean}
    f(x) = \frac{1}{B} \sum_{b=1}^{B} \alpha_b f_b(x)
\end{equation}

\begin{equation}\label{eq:boostingvote}
    f(x) = sign (\sum_{b=1}^{B} \alpha_b f_b(y | x))
\end{equation}

This technique assumes that the learners are 'weak' and homogeneous. \textit{Boosting} aims to build a stronger learner by combining a collection of \textit{weak learners}\index{Ensemble methods!boosting!weak learners}. \textit{Boosting} works in sequence and cannot be parallelized since the learners are dependent on each other. There are various other variations of boosting but one other popular implementation worth mentioning is \textit{Gradient Boosting}\index{Ensemble methods!boosting!gradient boosting} 'GBM' which focuses on optimizing the loss function using boosting \citep{breiman1997arcing}. 
\index{Ensemble methods!boosting|)}

\section{Stacking}
\index{Ensemble methods!stacking|(}
\textit{Stacking} or \textit{Stacked Generalization}\index{Ensemble methods!stacking!stacked generalization} is an ensemble method introduced by \citet{wolpert1992stacked}, it aims to reduce the generalization error of the combined model. Unlike the previous techniques it can be used to combine heterogeneous learners. In fact, this technique works better when the learners are different from each other and are known to be strong learners.  

In \textit{Stacking}, the dataset is first split into two; the training set and the test set. The training set is further split into K-folds like the K-fold cross validation technique. Each learner is then trained on $K-1$ folds and validation is done on the holdout fold. For prediction made by each learner at the holdout fold is recorded and stacked together to form a new dataset. The actual value/label is also stacked at the end. This procedure continues for $K$ times. Once all the learners have been trained using this method, a new dataset would be created with the predictions made by each learner which will be used as features. This dataset is then used to train the \textit{combiner learner}\index{Ensemble methods!stacking!combiner learner}, which is also referred to as the \textit{meta-model}\index{Ensemble methods!stacking!meta-model} or the \textit{aggregator model}\index{Ensemble methods!stacking!aggregator model} as shown in Figure~\ref{fig:stackingexample}. After this model is trained using this newly generated dataset it will be validated using the original test set. 

When using \textit{Stacking} for classification it is advised to use class probabilities instead of the predicted outcomes as it gives a better measure of confidence for each model, thus enhancing the meta-dataset \citep{ting1999issues}. 

\begin{figure}
  \includegraphics{graphics/ensemble_methods/stacking_example.JPG}
  \caption{
     A visual representation of the steps required when Stacking multiple machine learning models.
  }
  \label{fig:stackingexample}
\end{figure}

The general idea behind this ensemble technique is to combine multiple different learners together, but unlike the previous methods, \textit{Stacking} combines the models using another intermediate model which is trained on the predictions made by individual learners. \textit{Stacking} technique was also studied to identify Higgs bosons at the Large Hadron Collider where \textit{Stacking} outperformed \textit{Boosted Decision Trees}\index{Ensemble methods!stacking!boosted decision trees} \citep{alves2017stacking}.  
\index{Ensemble methods!stacking|)}

\index{Ensemble methods|)}